"""
æ­£ç¡®çš„MCP LangChainç¼–æ’
MCP Clientè¾“å…¥ â†’ Geminiæ„å›¾è¯†åˆ« â†’ GraphitiæŸ¥è¯¢Neo4j â†’ LLMé€‰æ‹©æœ€ä½³è·¯å¾„ â†’ Notionå†…å®¹è·å– â†’ è¾“å‡º
"""

import asyncio
import json
from typing import List, Dict, Any, Optional
from loguru import logger

from langchain.schema.runnable import RunnableLambda
from langchain.schema.runnable.base import RunnableSequence
from langchain.schema import BaseOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate

from .models import (
    IntentSearchRequest,
    IntentSearchResponse,
    IntentSearchMetadata,
    ConfidencePath,
    CorePageResult,
    RelatedPageResult,
    ConfidencePathMetadata
)
from .graphiti_client import GraphitiClient
from .notion_client import NotionExtractor
from config.settings import get_settings


class IntentKeywordsParser(BaseOutputParser[List[str]]):
    """æ„å›¾å…³é”®è¯è§£æå™¨"""
    
    def parse(self, text: str) -> List[str]:
        try:
            if '{' in text and '}' in text:
                start_idx = text.find('{')
                end_idx = text.rfind('}') + 1
                json_str = text[start_idx:end_idx]
                data = json.loads(json_str)
                return data.get('intent_keywords', [])
        except:
            pass
        
        # å¤‡é€‰è§£æ
        return [word.strip() for word in text.split() if len(word.strip()) > 1][:5]


class PathSelectionParser(BaseOutputParser[int]):
    """è·¯å¾„é€‰æ‹©è§£æå™¨"""
    
    def parse(self, text: str) -> int:
        try:
            if '{' in text and '}' in text:
                start_idx = text.find('{')
                end_idx = text.rfind('}') + 1
                json_str = text[start_idx:end_idx]
                data = json.loads(json_str)
                return data.get('selected_path_index', 1) - 1  # è½¬ä¸º0ç´¢å¼•
        except:
            pass
        return 0  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª


class CorrectMCPChain:
    """
    æ­£ç¡®çš„MCPæµç¨‹é“¾
    Geminiæ„å›¾è¯†åˆ« â†’ GraphitiæŸ¥è¯¢ â†’ LLMè·¯å¾„é€‰æ‹© â†’ Notionå†…å®¹è·å–
    """
    
    def __init__(self, graph_client: GraphitiClient, notion_client: NotionExtractor):
        self.graph_client = graph_client
        self.notion_client = notion_client
        self.settings = get_settings()
        
        # åˆå§‹åŒ–Geminiæ¨¡å‹
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            google_api_key=self.settings.gemini_api_key,
            temperature=0.1,
            max_output_tokens=2000
        )
        
        # è§£æå™¨
        self.intent_parser = IntentKeywordsParser()
        self.path_parser = PathSelectionParser()
        
        # æ„å»ºLangChainé“¾
        self.mcp_chain = self._build_mcp_chain()
    
    def _build_mcp_chain(self) -> RunnableSequence:
        """æ„å»ºMCPå¤„ç†é“¾"""
        
        # Step 1: Geminiæ„å›¾è¯†åˆ«é“¾
        intent_prompt = PromptTemplate(
            input_variables=["user_query"],
            template="""ä½œä¸ºæ™ºèƒ½æ„å›¾è¯†åˆ«ä¸“å®¶ï¼Œä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–å…³é”®çš„æ„å›¾è¯†åˆ«è¯æ±‡ã€‚

ç”¨æˆ·æŸ¥è¯¢: "{user_query}"

è¯·åˆ†ææŸ¥è¯¢å†…å®¹ï¼Œæå–å‡ºæœ€èƒ½ä»£è¡¨ç”¨æˆ·æœç´¢æ„å›¾çš„å…³é”®è¯ã€‚
è€ƒè™‘å› ç´ ï¼š
1. æ ¸å¿ƒæ¦‚å¿µå’Œä¸»é¢˜
2. å…·ä½“çš„åè¯å’Œä¸“ä¸šæœ¯è¯­
3. è¡ŒåŠ¨æ„å›¾ç›¸å…³è¯æ±‡

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "intent_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
    "reasoning": "ç®€çŸ­åˆ†æ"
}}

é™åˆ¶ï¼šæœ€å¤šæå–5ä¸ªå…³é”®è¯ã€‚"""
        )
        
        intent_chain = RunnableSequence(
            intent_prompt,
            self.llm,
            self.intent_parser,
            RunnableLambda(self._prepare_graphiti_query)
        )
        
        # Step 2: GraphitiæŸ¥è¯¢é“¾
        graphiti_chain = RunnableLambda(self._graphiti_query_paths)
        
        # Step 3: LLMè·¯å¾„é€‰æ‹©é“¾
        path_selection_prompt = PromptTemplate(
            input_variables=["user_query", "intent_keywords", "paths_info"],
            template="""ä½œä¸ºæ™ºèƒ½è·¯å¾„é€‰æ‹©ä¸“å®¶ï¼Œä»å€™é€‰è·¯å¾„ä¸­é€‰æ‹©æœ€ç¬¦åˆç”¨æˆ·æŸ¥è¯¢æ„å›¾çš„ä¸€æ¡ã€‚

ç”¨æˆ·æŸ¥è¯¢: "{user_query}"
æ„å›¾å…³é”®è¯: {intent_keywords}

å€™é€‰è·¯å¾„:
{paths_info}

è¯·åˆ†ææ¯æ¡è·¯å¾„ä¸ç”¨æˆ·æŸ¥è¯¢çš„åŒ¹é…ç¨‹åº¦ï¼Œè€ƒè™‘ï¼š
1. é¡µé¢æ ‡é¢˜çš„ç›¸å…³æ€§
2. å…³ç³»ç»“æ„çš„åˆç†æ€§  
3. å†…å®¹è¦†ç›–çš„å®Œæ•´æ€§
4. ç³»ç»Ÿç½®ä¿¡åº¦è¯„åˆ†

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "selected_path_index": 1,
    "reasoning": "é€‰æ‹©ç†ç”±"
}}

æ³¨æ„ï¼šselected_path_indexæ˜¯è·¯å¾„ç¼–å·ï¼ˆ1-Nï¼‰"""
        )
        
        path_selection_chain = RunnableSequence(
            RunnableLambda(self._build_path_selection_input),
            path_selection_prompt,
            self.llm,
            self.path_parser,
            RunnableLambda(self._select_best_path)
        )
        
        # Step 4: Notionå†…å®¹è·å–é“¾
        content_chain = RunnableLambda(self._fetch_notion_content)
        
        # Step 5: ç»“æœæ ¼å¼åŒ–é“¾
        format_chain = RunnableLambda(self._format_final_response)
        
        # ç»„åˆå®Œæ•´é“¾
        return RunnableSequence(
            intent_chain,
            graphiti_chain,
            path_selection_chain,
            content_chain,
            format_chain
        )
    
    async def process_mcp_request(self, user_query: str, client_id: str = "mcp_client") -> IntentSearchResponse:
        """å¤„ç†MCPè¯·æ±‚"""
        logger.info(f"ğŸ“¡ Processing MCP request: {user_query}")
        
        try:
            result = await self.mcp_chain.ainvoke({
                "user_query": user_query,
                "client_id": client_id,
                "graph_client": self.graph_client,
                "notion_client": self.notion_client
            })
            
            return result
            
        except Exception as e:
            logger.exception(f"MCP chain failed: {e}")
            return IntentSearchResponse(
                success=False,
                intent_keywords=[],
                confidence_paths=[],
                total_results=0,
                error=str(e)
            )
    
    def _prepare_graphiti_query(self, intent_keywords: List[str]) -> Dict[str, Any]:
        """å‡†å¤‡GraphitiæŸ¥è¯¢å‚æ•°"""
        return {
            "intent_keywords": intent_keywords,
            "graph_client": self.graph_client,
            "notion_client": self.notion_client
        }
    
    async def _graphiti_query_paths(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨GraphitiæŸ¥è¯¢Neo4jè·¯å¾„"""
        intent_keywords = inputs["intent_keywords"]
        graph_client = inputs["graph_client"]
        
        logger.info(f"ğŸ” GraphitiæŸ¥è¯¢å…³é”®è¯: {intent_keywords}")
        
        # å­˜å‚¨æ‰€æœ‰è·¯å¾„ç»“æœ
        all_paths = []
        
        for keyword in intent_keywords:
            try:
                # ä½¿ç”¨Graphitiæœç´¢
                search_results = await graph_client.search_by_query(keyword, limit=10)
                
                # ä¸ºæ¯ä¸ªæœç´¢ç»“æœæ„å»ºè·¯å¾„
                for result in search_results:
                    # ä»è¯¥é¡µé¢å¼€å§‹æ‰©å±•è·¯å¾„
                    expanded_pages = await graph_client.expand_from_pages(
                        page_ids=[result.notion_id],
                        depth=2,
                        relation_types=None  # ä½¿ç”¨æ‰€æœ‰å…³ç³»ç±»å‹
                    )
                    
                    # æ„å»ºè·¯å¾„æ•°æ®
                    path = {
                        "path_id": f"path_{result.notion_id}",
                        "core_page": {
                            "notion_id": result.notion_id,
                            "title": result.title,
                            "url": result.url,
                            "tags": result.tags,
                            "relevance_score": result.relevance_score
                        },
                        "related_pages": [
                            {
                                "notion_id": exp.page_id,
                                "title": exp.title,
                                "url": exp.url,
                                "depth": exp.depth,
                                "relationship_path": exp.path,
                                "tags": exp.tags
                            }
                            for exp in expanded_pages
                        ],
                        "total_pages": 1 + len(expanded_pages),
                        "keyword_match": keyword,
                        "confidence_score": result.relevance_score
                    }
                    
                    all_paths.append(path)
                    
            except Exception as e:
                logger.warning(f"GraphitiæŸ¥è¯¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
        
        # å»é‡å¹¶æŒ‰ç½®ä¿¡åº¦æ’åº
        unique_paths = {}
        for path in all_paths:
            path_id = path["path_id"]
            if path_id not in unique_paths or path["confidence_score"] > unique_paths[path_id]["confidence_score"]:
                unique_paths[path_id] = path
        
        sorted_paths = sorted(unique_paths.values(), key=lambda x: x["confidence_score"], reverse=True)[:5]
        
        logger.info(f"ğŸ“Š Graphitiè¿”å› {len(sorted_paths)} æ¡ä¼˜è´¨è·¯å¾„")
        
        return {
            **inputs,
            "paths": sorted_paths
        }
    
    def _build_path_selection_input(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºè·¯å¾„é€‰æ‹©è¾“å…¥"""
        user_query = inputs.get("user_query", "")
        intent_keywords = inputs.get("intent_keywords", [])
        paths = inputs.get("paths", [])
        
        # æ ¼å¼åŒ–è·¯å¾„ä¿¡æ¯
        paths_info = ""
        for i, path in enumerate(paths, 1):
            related_titles = [p["title"] for p in path["related_pages"][:3]]  # åªæ˜¾ç¤ºå‰3ä¸ª
            paths_info += f"""
è·¯å¾„ {i}: {path["path_id"]}
- æ ¸å¿ƒé¡µé¢: {path["core_page"]["title"]}
- ç›¸å…³é¡µé¢: {", ".join(related_titles)}{"..." if len(path["related_pages"]) > 3 else ""}
- æ€»é¡µé¢æ•°: {path["total_pages"]}
- ç³»ç»Ÿç½®ä¿¡åº¦: {path["confidence_score"]:.2f}
- åŒ¹é…å…³é”®è¯: {path["keyword_match"]}
"""
        
        return {
            "user_query": user_query,
            "intent_keywords": ", ".join(intent_keywords),
            "paths_info": paths_info.strip(),
            "paths": paths
        }
    
    def _select_best_path(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """é€‰æ‹©æœ€ä½³è·¯å¾„"""
        paths = inputs.get("paths", [])
        selected_index = inputs  # è¿™é‡Œinputså®é™…æ˜¯parserè¿”å›çš„index
        
        if isinstance(selected_index, dict):
            selected_index = 0  # å¤‡é€‰
        
        if 0 <= selected_index < len(paths):
            selected_path = paths[selected_index]
            logger.info(f"ğŸ† LLMé€‰æ‹©è·¯å¾„: {selected_path['path_id']}")
        else:
            selected_path = paths[0] if paths else None
            logger.warning(f"LLMé€‰æ‹©æ— æ•ˆï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè·¯å¾„")
        
        return {
            **inputs,
            "selected_path": selected_path
        }
    
    async def _fetch_notion_content(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–Notionå†…å®¹"""
        selected_path = inputs.get("selected_path")
        notion_client = inputs.get("notion_client")
        
        if not selected_path:
            return {**inputs, "content": None}
        
        logger.info(f"ğŸ“š è·å–è·¯å¾„å†…å®¹: {selected_path['path_id']}")
        
        # æ”¶é›†æ‰€æœ‰é¡µé¢ID
        all_page_ids = [selected_path["core_page"]["notion_id"]]
        all_page_ids.extend([p["notion_id"] for p in selected_path["related_pages"]])
        
        try:
            # æ‰¹é‡è·å–é¡µé¢å†…å®¹
            page_contents = await notion_client.get_pages_content_batch(all_page_ids)
            
            # æ„å»ºå†…å®¹ç»“æœ
            content_result = {
                "core_page": {
                    **selected_path["core_page"],
                    "content": page_contents.get(selected_path["core_page"]["notion_id"], "")
                },
                "related_pages": []
            }
            
            for related in selected_path["related_pages"]:
                content_result["related_pages"].append({
                    **related,
                    "content": page_contents.get(related["notion_id"], "")
                })
            
            logger.info(f"âœ… æˆåŠŸè·å– {len(all_page_ids)} ä¸ªé¡µé¢å†…å®¹")
            
            return {
                **inputs,
                "content": content_result
            }
            
        except Exception as e:
            logger.error(f"è·å–Notionå†…å®¹å¤±è´¥: {e}")
            return {**inputs, "content": None}
    
    def _format_final_response(self, inputs: Dict[str, Any]) -> IntentSearchResponse:
        """æ ¼å¼åŒ–æœ€ç»ˆå“åº”"""
        user_query = inputs.get("user_query", "")
        client_id = inputs.get("client_id", "mcp_client")
        intent_keywords = inputs.get("intent_keywords", [])
        paths = inputs.get("paths", [])
        content = inputs.get("content")
        
        if not content:
            return IntentSearchResponse(
                success=False,
                intent_keywords=intent_keywords,
                confidence_paths=[],
                total_results=0,
                error="æœªèƒ½è·å–å†…å®¹"
            )
        
        # æ„å»ºConfidencePath
        core_page_result = CorePageResult(
            notion_id=content["core_page"]["notion_id"],
            title=content["core_page"]["title"],
            url=content["core_page"]["url"],
            tags=content["core_page"]["tags"],
            content=content["core_page"]["content"],
            confidence_score=content["core_page"]["relevance_score"]
        )
        
        related_page_results = []
        for related in content["related_pages"]:
            related_page_results.append(RelatedPageResult(
                page_id=related["notion_id"],
                title=related["title"],
                url=related["url"],
                content=related["content"],
                depth=related["depth"],
                relationship_path=related["relationship_path"]
            ))
        
        confidence_path = ConfidencePath(
            core_page=core_page_result,
            related_pages=related_page_results,
            path_metadata=ConfidencePathMetadata(
                total_pages=1 + len(related_page_results),
                confidence_level="high" if content["core_page"]["relevance_score"] >= 0.8 else "medium",
                expansion_depth=2
            )
        )
        
        return IntentSearchResponse(
            success=True,
            intent_keywords=intent_keywords,
            confidence_paths=[confidence_path],
            total_results=1,
            search_metadata=IntentSearchMetadata(
                initial_candidates=len(paths),
                high_confidence_matches=1,
                confidence_threshold=0.7
            )
        )


# å…¨å±€å®ä¾‹
_correct_mcp_chain = None

def get_correct_mcp_chain(graph_client: GraphitiClient, notion_client: NotionExtractor) -> CorrectMCPChain:
    """è·å–æ­£ç¡®çš„MCPé“¾å®ä¾‹"""
    global _correct_mcp_chain
    if _correct_mcp_chain is None:
        _correct_mcp_chain = CorrectMCPChain(graph_client, notion_client)
    return _correct_mcp_chain