#!/usr/bin/env python3
"""
å¾®ä¿¡æ•°æ®åŒæ­¥è„šæœ¬
å°†JSONæ ¼å¼çš„å¾®ä¿¡èŠå¤©æ•°æ®è½¬æ¢ä¸ºGraphiti Episodeså¹¶å­˜å‚¨åˆ°Neo4j
"""

import asyncio
import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Any
from loguru import logger
from tqdm import tqdm

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
PROJECT_ROOT = Path(__file__).parent.parent.absolute()
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from core.wechat_processor import WeChatDataProcessor, process_wechat_data
from core.wechat_models import WeChatEpisode, EpisodeGenerationResult
from config.settings import settings


class WeChatSyncScript:
    """å¾®ä¿¡æ•°æ®åŒæ­¥è„šæœ¬ä¸»ç±»"""
    
    def __init__(self, data_path: str = None):
        if data_path:
            # å¦‚æœæä¾›çš„æ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½•è§£æ
            if not Path(data_path).is_absolute():
                self.data_path = PROJECT_ROOT / data_path
            else:
                self.data_path = Path(data_path)
        else:
            # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½• - å¾®ä¿¡æ•°æ®æ ¹ç›®å½•
            self.data_path = PROJECT_ROOT / "local_data" / "wechat"
        
        self.processed_files_path = PROJECT_ROOT / "local_data" / "wechat" / "processed_wechat_files.txt"
        self.processor = WeChatDataProcessor()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.data_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"å¾®ä¿¡æ•°æ®åŒæ­¥è„šæœ¬åˆå§‹åŒ–å®Œæˆï¼Œå¾®ä¿¡æ•°æ®æ ¹ç›®å½•: {self.data_path}")
        logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
        logger.info(f"ç»å¯¹æ•°æ®è·¯å¾„: {self.data_path.absolute()}")
        logger.info(f"ç›®å½•æ˜¯å¦å­˜åœ¨: {self.data_path.exists()}")
    
    async def sync_all(self, force: bool = False) -> EpisodeGenerationResult:
        """åŒæ­¥æŒ‡å®šæ–‡ä»¶å¤¹çš„æ‰€æœ‰JSONæ–‡ä»¶"""
        logger.info(f"å¼€å§‹{'å¼ºåˆ¶' if force else 'å¢é‡'}åŒæ­¥å¾®ä¿¡æ•°æ®")
        logger.info(f"å¤„ç†ç›®å½•: {self.data_path}")
        
        try:
            # ç›´æ¥å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„JSONæ–‡ä»¶
            return await self._sync_folder(self.data_path, force)
            
        except Exception as e:
            logger.error(f"åŒæ­¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return EpisodeGenerationResult(
                success=False,
                errors=[str(e)]
            )
    
    async def _sync_folder(self, folder_path: Path, force: bool = False) -> EpisodeGenerationResult:
        """åŒæ­¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶"""
        try:
            # è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
            json_files = list(folder_path.glob("*.json"))
            logger.info(f"å‘ç° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
            
            if not force:
                # å¢é‡åŒæ­¥ï¼šè¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
                processed_files = self._get_processed_files()
                original_count = len(json_files)
                json_files = [f for f in json_files if f.name not in processed_files]
                logger.info(f"è¿‡æ»¤åå‰©ä½™ {len(json_files)} ä¸ªæœªå¤„ç†æ–‡ä»¶ (åŸæœ‰ {original_count} ä¸ª)")
            
            if not json_files:
                logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶")
                return EpisodeGenerationResult(success=True, total_episodes=0)
            
            # æ˜¾ç¤ºå³å°†å¤„ç†çš„æ–‡ä»¶
            logger.info(f"å³å°†å¤„ç†çš„æ–‡ä»¶: {', '.join([f.name for f in json_files])}")
            
            # åˆå§‹åŒ–Graphitiå®¢æˆ·ç«¯
            await self.processor.client.initialize()
            
            # å¤„ç†æŒ‡å®šçš„æ–‡ä»¶åˆ—è¡¨ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
            with tqdm(total=len(json_files), desc="å¤„ç†JSONæ–‡ä»¶", unit="æ–‡ä»¶") as pbar:
                processed_count = 0
                
                def file_processed_callback(filename: str):
                    """æ¯å¤„ç†å®Œä¸€ä¸ªæ–‡ä»¶çš„å›è°ƒå‡½æ•°"""
                    nonlocal processed_count
                    processed_count += 1
                    pbar.update(1)
                    pbar.set_description(f"å·²å¤„ç† {processed_count}/{len(json_files)} ä¸ªæ–‡ä»¶")
                    # ç«‹å³è®°å½•å·²å¤„ç†çš„æ–‡ä»¶
                    self._mark_file_processed(filename)
                
                # å¤„ç†æŒ‡å®šçš„æ–‡ä»¶åˆ—è¡¨
                episodes, processed_files = await self.processor.process_specific_files(json_files, file_processed_callback)
                
                if not episodes:
                    logger.warning("æ²¡æœ‰Episodeéœ€è¦å­˜å‚¨")
                    result = EpisodeGenerationResult(
                        success=True,
                        total_episodes=0,
                        episodes_by_type={},
                        processed_files=processed_files
                    )
                else:
                    # æ‰¹é‡å­˜å‚¨åˆ°Neo4j
                    logger.info(f"å¼€å§‹å­˜å‚¨ {len(episodes)} ä¸ªEpisodeåˆ°Neo4j")
                    result = await self.processor.client.add_graphiti_episodes_bulk(episodes)
                    
                    if result.success:
                        logger.info(f"æˆåŠŸå­˜å‚¨ {result.total_episodes} ä¸ªEpisodeåˆ°Neo4j")
                        result.processed_files = processed_files
                    else:
                        logger.error(f"å­˜å‚¨Episodeå¤±è´¥: {result.errors}")
                        # å­˜å‚¨å¤±è´¥æ—¶ï¼Œéœ€è¦ä»txtæ–‡ä»¶ä¸­ç§»é™¤å·²è®°å½•çš„æ–‡ä»¶ï¼ˆå› ä¸ºå®ƒä»¬å®é™…æ²¡æœ‰æˆåŠŸå­˜å‚¨ï¼‰
                        result.processed_files = []
                        logger.warning("å› å­˜å‚¨å¤±è´¥ï¼Œéœ€è¦é‡æ–°å¤„ç†è¿™äº›æ–‡ä»¶")
            
            # å¤„ç†ç»“æœ
            if result.success:
                logger.info(f"âœ… æˆåŠŸå¤„ç† {len(result.processed_files)} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {result.total_episodes} ä¸ªEpisode")
            else:
                logger.error(f"âŒ å¤„ç†å¤±è´¥ï¼š{result.errors}")
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤¹å¤±è´¥: {e}")
            return EpisodeGenerationResult(
                success=False,
                errors=[str(e)]
            )
    
    async def validate_files(self) -> Dict[str, Any]:
        """éªŒè¯JSONæ–‡ä»¶æ ¼å¼"""
        logger.info("å¼€å§‹éªŒè¯JSONæ–‡ä»¶æ ¼å¼")
        
        json_files = list(self.data_path.glob("**/*.json"))
        validation_results = {
            "total_files": len(json_files),
            "valid_files": [],
            "invalid_files": [],
            "errors": []
        }
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # åŸºæœ¬æ ¼å¼éªŒè¯
                if self._validate_json_format(data):
                    validation_results["valid_files"].append(str(json_file))
                    logger.debug(f"æ–‡ä»¶æ ¼å¼æ­£ç¡®: {json_file}")
                else:
                    validation_results["invalid_files"].append(str(json_file))
                    logger.warning(f"æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®: {json_file}")
                    
            except Exception as e:
                validation_results["invalid_files"].append(str(json_file))
                validation_results["errors"].append(f"{json_file}: {str(e)}")
                logger.error(f"æ–‡ä»¶éªŒè¯å¤±è´¥ {json_file}: {e}")
        
        logger.info(f"éªŒè¯å®Œæˆï¼š{validation_results['total_files']} ä¸ªæ–‡ä»¶ä¸­æœ‰ {len(validation_results['valid_files'])} ä¸ªæ ¼å¼æ­£ç¡®")
        return validation_results
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥çŠ¶æ€"""
        json_files = list(self.data_path.glob("**/*.json"))
        processed_files = self._get_processed_files()
        
        status = {
            "data_path": str(self.data_path),
            "total_files": len(json_files),
            "processed_files": len(processed_files),
            "unprocessed_files": len(json_files) - len(processed_files),
            "processed_file_list": list(processed_files),
            "unprocessed_file_list": [str(f) for f in json_files if f.name not in processed_files]
        }
        
        return status
    
    def _get_processed_files(self) -> set:
        """è·å–å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        if not self.processed_files_path.exists():
            return set()
        
        try:
            with open(self.processed_files_path, 'r', encoding='utf-8') as f:
                return set(line.strip() for line in f if line.strip())
        except Exception as e:
            logger.warning(f"è¯»å–å·²å¤„ç†æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return set()
    
    def _mark_file_processed(self, filename: str):
        """æ ‡è®°å•ä¸ªæ–‡ä»¶ä¸ºå·²å¤„ç†"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»è®°å½•è¿‡
            processed_files = self._get_processed_files()
            if filename not in processed_files:
                with open(self.processed_files_path, 'a', encoding='utf-8') as f:
                    f.write(f"{filename}\n")
                logger.debug(f"æ ‡è®°æ–‡ä»¶ {filename} ä¸ºå·²å¤„ç†")
        except Exception as e:
            logger.error(f"æ ‡è®°æ–‡ä»¶ {filename} ä¸ºå·²å¤„ç†å¤±è´¥: {e}")
    
    def _mark_files_processed(self, file_paths: List[str]):
        """æ ‡è®°å¤šä¸ªæ–‡ä»¶ä¸ºå·²å¤„ç†"""
        try:
            processed_files = self._get_processed_files()
            new_files = []
            
            for file_path in file_paths:
                # æå–æ–‡ä»¶å
                filename = Path(file_path).name
                if filename not in processed_files:
                    new_files.append(filename)
            
            if new_files:
                with open(self.processed_files_path, 'a', encoding='utf-8') as f:
                    for filename in new_files:
                        f.write(f"{filename}\n")
                logger.debug(f"æ ‡è®° {len(new_files)} ä¸ªæ–°æ–‡ä»¶ä¸ºå·²å¤„ç†")
        except Exception as e:
            logger.error(f"æ ‡è®°æ–‡ä»¶å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
    
    def _validate_json_format(self, data: Any) -> bool:
        """éªŒè¯JSONæ ¼å¼æ˜¯å¦æ­£ç¡®"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ—è¡¨ï¼ˆèŠå¤©æ¶ˆæ¯æ ¼å¼ï¼‰
            if isinstance(data, list):
                # éªŒè¯æ¶ˆæ¯æ ¼å¼
                for message in data:
                    if not isinstance(message, dict):
                        return False
                    if "sender" not in message:
                        return False
                return True
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å­—å…¸ï¼ˆç»“æ„åŒ–æ ¼å¼ï¼‰
            elif isinstance(data, dict):
                # å¯ä»¥æ·»åŠ æ›´å¤šéªŒè¯é€»è¾‘
                return True
            
            return False
            
        except Exception:
            return False
    
    def _count_episodes_by_type(self, episodes: List[WeChatEpisode]) -> Dict[str, int]:
        """æŒ‰ç±»å‹ç»Ÿè®¡Episodeæ•°é‡"""
        counts = {}
        for episode in episodes:
            episode_type = episode.episode_type.value
            counts[episode_type] = counts.get(episode_type, 0) + 1
        return counts
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.processor.close()


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¾®ä¿¡æ•°æ®åŒæ­¥è„šæœ¬")
    parser.add_argument("command", choices=["sync", "force", "validate", "status"], 
                       help="æ‰§è¡Œçš„å‘½ä»¤")
    parser.add_argument("--data-path", type=str,
                       help="è¦å¤„ç†çš„æ•°æ®ç›®å½•è·¯å¾„ï¼Œä¾‹å¦‚: local_data/wechat/group", default="local_data/wechat/person")
    parser.add_argument("--log-level", default="INFO", 
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(sys.stderr, level=args.log_level)
    
    # åˆå§‹åŒ–åŒæ­¥è„šæœ¬
    sync_script = WeChatSyncScript(args.data_path)
    
    try:
        if args.command == "sync":
            # å¢é‡åŒæ­¥
            result = await sync_script.sync_all(force=False)
            if result.success:
                print(f"âœ… å¢é‡åŒæ­¥æˆåŠŸï¼šå¤„ç†äº† {len(result.processed_files)} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {result.total_episodes} ä¸ªEpisode")
                if result.episodes_by_type:
                    print("Episodeç±»å‹åˆ†å¸ƒï¼š")
                    for episode_type, count in result.episodes_by_type.items():
                        print(f"  - {episode_type}: {count}")
            else:
                print(f"âŒ å¢é‡åŒæ­¥å¤±è´¥ï¼š{result.errors}")
                sys.exit(1)
        
        elif args.command == "force":
            # å¼ºåˆ¶åŒæ­¥
            result = await sync_script.sync_all(force=True)
            if result.success:
                print(f"âœ… å¼ºåˆ¶åŒæ­¥æˆåŠŸï¼šå¤„ç†äº† {len(result.processed_files)} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {result.total_episodes} ä¸ªEpisode")
                if result.episodes_by_type:
                    print("Episodeç±»å‹åˆ†å¸ƒï¼š")
                    for episode_type, count in result.episodes_by_type.items():
                        print(f"  - {episode_type}: {count}")
            else:
                print(f"âŒ å¼ºåˆ¶åŒæ­¥å¤±è´¥ï¼š{result.errors}")
                sys.exit(1)
        
        elif args.command == "validate":
            # éªŒè¯æ–‡ä»¶æ ¼å¼
            validation_results = await sync_script.validate_files()
            print(f"ğŸ“‹ æ–‡ä»¶æ ¼å¼éªŒè¯ç»“æœï¼š")
            print(f"  æ€»æ–‡ä»¶æ•°: {validation_results['total_files']}")
            print(f"  æœ‰æ•ˆæ–‡ä»¶: {len(validation_results['valid_files'])}")
            print(f"  æ— æ•ˆæ–‡ä»¶: {len(validation_results['invalid_files'])}")
            
            if validation_results['invalid_files']:
                print("\nâŒ æ— æ•ˆæ–‡ä»¶åˆ—è¡¨ï¼š")
                for invalid_file in validation_results['invalid_files']:
                    print(f"  - {invalid_file}")
            
            if validation_results['errors']:
                print("\nâš ï¸  é”™è¯¯è¯¦æƒ…ï¼š")
                for error in validation_results['errors']:
                    print(f"  - {error}")
        
        elif args.command == "status":
            # æŸ¥çœ‹çŠ¶æ€
            status = sync_script.get_status()
            print(f"ğŸ“Š åŒæ­¥çŠ¶æ€ï¼š")
            print(f"  æ•°æ®ç›®å½•: {status['data_path']}")
            print(f"  æ€»æ–‡ä»¶æ•°: {status['total_files']}")
            print(f"  å·²å¤„ç†æ–‡ä»¶: {status['processed_files']}")
            print(f"  æœªå¤„ç†æ–‡ä»¶: {status['unprocessed_files']}")
            
            if status['unprocessed_files'] > 0:
                print("\nğŸ“ æœªå¤„ç†æ–‡ä»¶åˆ—è¡¨ï¼š")
                for unprocessed_file in status['unprocessed_file_list']:
                    print(f"  - {unprocessed_file}")
            
            if status['processed_files'] > 0:
                print(f"\nâœ… å·²å¤„ç†æ–‡ä»¶: {status['processed_files']} ä¸ª")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    finally:
        # æ¸…ç†èµ„æº
        await sync_script.cleanup()


if __name__ == "__main__":
    asyncio.run(main())