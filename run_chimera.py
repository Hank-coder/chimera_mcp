#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chimera åŒæ­¥æœåŠ¡å¯åŠ¨è„šæœ¬
ä¸“é—¨ç”¨äºå¯åŠ¨15åˆ†é’Ÿå®šæœŸåŒæ­¥ç›‘æµ‹
"""

import asyncio
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime

from sync_service.sync_service import SyncService

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


from config.logging import setup_logging
from config.settings import get_settings
from loguru import logger


async def generate_cache():
    """ç”ŸæˆJSONç¼“å­˜æ–‡ä»¶"""
    try:
        logger.info("ğŸ“„ ç”ŸæˆJSONç¼“å­˜...")
        cache_dir = Path("llm_cache")
        cache_dir.mkdir(exist_ok=True)
        cache_file = cache_dir / "chimera_cache.json"
        
        # åˆå§‹åŒ–GraphitiClient
        from core.graphiti_client import GraphitiClient
        graph_client = GraphitiClient()
        await graph_client.initialize()
        
        # æŸ¥è¯¢æ‰€æœ‰é¡µé¢å’ŒCHILD_OFå…³ç³»ç”¨äºæ„å»ºè·¯å¾„
        query = """
        MATCH (p:NotionPage)
        OPTIONAL MATCH (p)-[:CHILD_OF]->(parent:NotionPage)
        OPTIONAL MATCH (child:NotionPage)-[:CHILD_OF]->(p)
        RETURN p {
            .notionId,
            .title,
            .type,
            .tags,
            .lastEditedTime,
            .url,
            .level
        } as page,
        parent.notionId as parent_id,
        collect(DISTINCT child.notionId) as children_ids
        """
        
        cache_data = {
            "generated_at": datetime.now().isoformat(),
            "pages": {},
            "paths": [],
            "metadata": {
                "total_pages": 0,
                "total_paths": 0
            }
        }
        
        pages_map = {}
        
        async with graph_client._driver.session() as session:
            result = await session.run(query)
            
            async for record in result:
                page = record["page"]
                parent_id = record["parent_id"]
                children_ids = record["children_ids"] or []
                
                # å¤„ç†DateTimeåºåˆ—åŒ–
                last_edited = page["lastEditedTime"]
                if hasattr(last_edited, 'isoformat'):
                    last_edited = last_edited.isoformat()
                elif last_edited:
                    last_edited = str(last_edited)
                
                cache_data["pages"][page["notionId"]] = {
                    "title": page["title"],
                    "type": page["type"],
                    "tags": page["tags"] or [],
                    "lastEditedTime": last_edited,
                    "url": page["url"],
                    "level": page.get("level", 0),
                    "parent_id": parent_id,
                    "children_ids": children_ids
                }
                pages_map[page["notionId"]] = cache_data["pages"][page["notionId"]]
        
        # æ„å»ºå®Œæ•´è·¯å¾„
        def build_paths():
            paths = []
            
            # æ‰¾åˆ°æ‰€æœ‰å¶å­èŠ‚ç‚¹ï¼ˆæ²¡æœ‰å­èŠ‚ç‚¹çš„èŠ‚ç‚¹ï¼‰
            leaf_nodes = [pid for pid, page in pages_map.items() if not page["children_ids"]]
            
            for leaf_id in leaf_nodes:
                # ä»å¶å­èŠ‚ç‚¹å‘ä¸Šæ„å»ºè·¯å¾„
                path_ids = []
                path_titles = []
                current_id = leaf_id
                
                while current_id and current_id in pages_map:
                    page = pages_map[current_id]
                    path_ids.insert(0, current_id)
                    path_titles.insert(0, page["title"])
                    current_id = page["parent_id"]
                
                if len(path_ids) > 0:
                    path_string = " -> ".join(path_titles)
                    paths.append({
                        "path_string": path_string,
                        "path_titles": path_titles,
                        "path_ids": path_ids,
                        "leaf_id": leaf_id,
                        "leaf_title": pages_map[leaf_id]["title"],
                        "path_length": len(path_ids) - 1
                    })
            
            return paths
        
        cache_data["paths"] = build_paths()
        cache_data["metadata"]["total_pages"] = len(cache_data["pages"])
        cache_data["metadata"]["total_paths"] = len(cache_data["paths"])
        
        # è‡ªå®šä¹‰JSONç¼–ç å™¨å¤„ç†ç‰¹æ®Šç±»å‹
        def json_encoder(obj):
            if hasattr(obj, 'isoformat'):  # DateTimeå¯¹è±¡
                return obj.isoformat()
            elif hasattr(obj, '__dict__'):  # å…¶ä»–å¯¹è±¡
                return str(obj)
            return obj
        
        # åŸå­å†™å…¥
        temp_file = cache_file.with_suffix('.tmp')
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2, default=json_encoder)
        
        temp_file.replace(cache_file)
        logger.info(f"âœ… ç¼“å­˜ç”Ÿæˆå®Œæˆï¼š{cache_data['metadata']['total_pages']} é¡µé¢ï¼Œ{cache_data['metadata']['total_paths']} è·¯å¾„")
        
        await graph_client.close()
        
    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜ç”Ÿæˆå¤±è´¥: {e}")


async def run_continuous_sync():
    """è¿è¡ŒæŒç»­çš„15åˆ†é’ŸåŒæ­¥ç›‘æµ‹"""
    logger.info("ğŸ”„ å¯åŠ¨15åˆ†é’ŸåŒæ­¥ç›‘æµ‹æœåŠ¡...")
    
    sync_service = SyncService()
    
    try:
        await sync_service.initialize()
        logger.info("âœ… åŒæ­¥æœåŠ¡å·²å¯åŠ¨ï¼Œæ¯15åˆ†é’Ÿæ£€æŸ¥æ›´æ–°")
        
        while True:
            try:
                logger.info("ğŸ” å¼€å§‹æ£€æŸ¥Notionæ›´æ–°...")
                success = await sync_service.run_manual_sync()
                
                if success:
                    logger.info("âœ… åŒæ­¥æ£€æŸ¥å®Œæˆ")
                    # ç”ŸæˆJSONç¼“å­˜
                    await generate_cache()
                else:
                    logger.warning("âš ï¸ åŒæ­¥æ£€æŸ¥å‘ç°é—®é¢˜")
                
                # ç­‰å¾…15åˆ†é’Ÿ
                logger.info("â³ ç­‰å¾…15åˆ†é’Ÿåè¿›è¡Œä¸‹æ¬¡æ£€æŸ¥...")
                await asyncio.sleep(15 * 60)  # 15åˆ†é’Ÿ
                    
            except Exception as e:
                logger.exception(f"âŒ åŒæ­¥ç›‘æµ‹å¼‚å¸¸: {e}")
                # é‡åˆ°å¼‚å¸¸ç­‰å¾…5åˆ†é’Ÿåé‡è¯•
                logger.info("â³ ç­‰å¾…5åˆ†é’Ÿåé‡è¯•...")
                await asyncio.sleep(5 * 60)
                
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
    finally:
        await sync_service.stop()
        logger.info("âœ… åŒæ­¥æœåŠ¡å·²åœæ­¢")


async def run_manual_sync():
    """è¿è¡Œä¸€æ¬¡æ€§æ‰‹åŠ¨åŒæ­¥"""
    logger.info("âš¡ æ‰§è¡Œæ‰‹åŠ¨åŒæ­¥...")
    sync_service = SyncService()
    try:
        await sync_service.initialize()
        success = await sync_service.run_manual_sync()
        if success:
            logger.info("âœ… æ‰‹åŠ¨åŒæ­¥å®Œæˆ")
            # ç”ŸæˆJSONç¼“å­˜
            await generate_cache()
        else:
            logger.error("âŒ æ‰‹åŠ¨åŒæ­¥å¤±è´¥")
            sys.exit(1)
    finally:
        await sync_service.stop()


async def show_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    logger.info("ğŸ“Š æ£€æŸ¥ç³»ç»ŸçŠ¶æ€...")
    
    try:
        sync_service = SyncService()
        await sync_service.initialize()
        
        stats = await sync_service.get_stats()
        logger.info(f"åŒæ­¥æœåŠ¡çŠ¶æ€: {stats}")
        
        await sync_service.stop()
        
    except Exception as e:
        logger.error(f"æ— æ³•è·å–çŠ¶æ€: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Chimera åŒæ­¥æœåŠ¡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python run_chimera.py                    # è¿è¡Œ15åˆ†é’ŸæŒç»­åŒæ­¥ç›‘æµ‹
  python run_chimera.py --manual-sync      # æ‰§è¡Œä¸€æ¬¡æ‰‹åŠ¨åŒæ­¥
  python run_chimera.py --status           # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€

æ³¨æ„: MCPæœåŠ¡å™¨è¯·å•ç‹¬è¿è¡Œï¼š
  python fastmcp_server.py --port 3000
        """
    )
    
    parser.add_argument(
        "--manual-sync", 
        action="store_true",
        help="æ‰§è¡Œä¸€æ¬¡æ‰‹åŠ¨åŒæ­¥"
    )
    
    parser.add_argument(
        "--status", 
        action="store_true",
        help="æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"
    )
    
    parser.add_argument(
        "--debug", 
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ¨¡å¼"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸ”„ Chimera åŒæ­¥æœåŠ¡")
    logger.info("=" * 60)
    
    # æ£€æŸ¥é…ç½®
    settings = get_settings()
    if args.debug:
        logger.info(f"é…ç½®: Neo4j URI: {settings.neo4j_uri}")
    
    # æ ¹æ®å‚æ•°è¿è¡Œç›¸åº”çš„åŠŸèƒ½
    try:
        if args.manual_sync:
            asyncio.run(run_manual_sync())
        elif args.status:
            asyncio.run(show_status())
        else:
            # é»˜è®¤è¿è¡ŒæŒç»­åŒæ­¥
            asyncio.run(run_continuous_sync())
            
    except Exception as e:
        logger.exception(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()