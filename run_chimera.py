#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chimera åŒæ­¥æœåŠ¡å¯åŠ¨è„šæœ¬
ä¸“é—¨ç”¨äºå¯åŠ¨15åˆ†é’Ÿå®šæœŸåŒæ­¥ç›‘æµ‹
"""

import asyncio
import sys
import argparse
from pathlib import Path

from sync_service.sync_service import SyncService

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


from config.logging import setup_logging
from config.settings import get_settings
from loguru import logger


async def generate_cache():
    """ç”ŸæˆJSONç¼“å­˜æ–‡ä»¶ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸å†å¤„ç†åˆ é™¤æ£€æµ‹ï¼‰"""
    import json
    from pathlib import Path
    from datetime import datetime
    from core.graphiti_client import GraphitiClient
    
    try:
        logger.info("ğŸ“„ ç”ŸæˆJSONç¼“å­˜...")
        
        graph_client = GraphitiClient()
        await graph_client.initialize()
        
        # ç”Ÿæˆç¼“å­˜æ•°æ®
        cache_data = await _build_cache_data(graph_client)
        
        # å†™å…¥ç¼“å­˜æ–‡ä»¶
        cache_dir = Path("llm_cache")
        cache_dir.mkdir(exist_ok=True)
        cache_file = cache_dir / "chimera_cache.json"
        
        _write_cache_file(cache_file, cache_data)
        
        await graph_client.close()
        
        logger.info(f"âœ… ç¼“å­˜å®Œæˆï¼š{cache_data['metadata']['total_pages']} é¡µé¢ï¼Œ{cache_data['metadata']['total_paths']} è·¯å¾„")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜ç”Ÿæˆå¤±è´¥: {e}")
        return False


async def _build_cache_data(graph_client):
    """æ„å»ºç¼“å­˜æ•°æ®"""
    from datetime import datetime
    
    # æŸ¥è¯¢æ‰€æœ‰é¡µé¢å’Œå…³ç³»
    query = """
    MATCH (p:NotionPage)
    OPTIONAL MATCH (p)-[:CHILD_OF]->(parent:NotionPage)
    OPTIONAL MATCH (child:NotionPage)-[:CHILD_OF]->(p)
    RETURN p {
        .notionId,
        .title,
        .type,
        .tags,
        .lastEditedTime,
        .url,
        .level
    } as page,
    parent.notionId as parent_id,
    collect(DISTINCT child.notionId) as children_ids
    """
    
    cache_data = {
        "generated_at": datetime.now().isoformat(),
        "pages": {},
        "paths": [],
        "metadata": {"total_pages": 0, "total_paths": 0}
    }
    
    pages_map = {}
    
    async with graph_client._driver.session() as session:
        result = await session.run(query)
        
        async for record in result:
            page = record["page"]
            parent_id = record["parent_id"]
            children_ids = record["children_ids"] or []
            
            # å¤„ç†DateTimeåºåˆ—åŒ–
            last_edited = page["lastEditedTime"]
            if hasattr(last_edited, 'isoformat'):
                last_edited = last_edited.isoformat()
            elif last_edited:
                last_edited = str(last_edited)
            
            cache_data["pages"][page["notionId"]] = {
                "title": page["title"],
                "type": page["type"],
                "tags": page["tags"] or [],
                "lastEditedTime": last_edited,
                "url": page["url"],
                "level": page.get("level", 0),
                "parent_id": parent_id,
                "children_ids": children_ids
            }
            pages_map[page["notionId"]] = cache_data["pages"][page["notionId"]]
    
    # æ„å»ºè·¯å¾„
    cache_data["paths"] = _build_paths(pages_map)
    cache_data["metadata"]["total_pages"] = len(cache_data["pages"])
    cache_data["metadata"]["total_paths"] = len(cache_data["paths"])
    
    return cache_data


def _build_paths(pages_map):
    """æ„å»ºå®Œæ•´è·¯å¾„"""
    paths = []
    
    # æ‰¾åˆ°æ‰€æœ‰å¶å­èŠ‚ç‚¹
    leaf_nodes = [pid for pid, page in pages_map.items() if not page["children_ids"]]
    
    for leaf_id in leaf_nodes:
        # ä»å¶å­èŠ‚ç‚¹å‘ä¸Šæ„å»ºè·¯å¾„
        path_ids = []
        path_titles = []
        current_id = leaf_id
        
        while current_id and current_id in pages_map:
            page = pages_map[current_id]
            path_ids.insert(0, current_id)
            path_titles.insert(0, page["title"])
            current_id = page["parent_id"]
        
        if len(path_ids) > 0:
            path_string = " -> ".join(path_titles)
            paths.append({
                "path_string": path_string,
                "path_titles": path_titles,
                "path_ids": path_ids,
                "leaf_id": leaf_id,
                "leaf_title": pages_map[leaf_id]["title"],
                "path_length": len(path_ids) - 1
            })
    
    return paths


def _write_cache_file(cache_file, cache_data):
    """å†™å…¥ç¼“å­˜æ–‡ä»¶"""
    import json
    
    def json_encoder(obj):
        if hasattr(obj, 'isoformat'):
            return obj.isoformat()
        elif hasattr(obj, '__dict__'):
            return str(obj)
        return obj
    
    # åŸå­å†™å…¥
    temp_file = cache_file.with_suffix('.tmp')
    with open(temp_file, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2, default=json_encoder)
    
    temp_file.replace(cache_file)


async def run_continuous_sync():
    """è¿è¡ŒæŒç»­çš„åŒæ­¥ç›‘æµ‹"""
    settings = get_settings()
    sync_interval = settings.sync_interval_minutes
    
    logger.info(f"ğŸ”„ å¯åŠ¨{sync_interval}åˆ†é’ŸåŒæ­¥ç›‘æµ‹æœåŠ¡...")
    
    sync_service = SyncService()
    sync_cycle_count = 0
    
    try:
        await sync_service.initialize()
        logger.info(f"âœ… åŒæ­¥æœåŠ¡å·²å¯åŠ¨ï¼Œæ¯{sync_interval}åˆ†é’Ÿæ£€æŸ¥æ›´æ–°")
        
        while True:
            try:
                logger.info("ğŸ” å¼€å§‹æ£€æŸ¥Notionæ›´æ–°...")
                success = await sync_service.run_manual_sync()
                
                if success:
                    logger.info("âœ… åŒæ­¥æ£€æŸ¥å®Œæˆ")
                    # ç”ŸæˆJSONç¼“å­˜
                    await generate_cache()
                else:
                    logger.warning("âš ï¸ åŒæ­¥æ£€æŸ¥å‘ç°é—®é¢˜")
                
                sync_cycle_count += 1

                # ç­‰å¾…é…ç½®çš„æ—¶é—´é—´éš”
                logger.info(f"â³ ç­‰å¾…{sync_interval}åˆ†é’Ÿåè¿›è¡Œä¸‹æ¬¡æ£€æŸ¥...")
                await asyncio.sleep(sync_interval * 60)
                    
            except Exception as e:
                logger.exception(f"âŒ åŒæ­¥ç›‘æµ‹å¼‚å¸¸: {e}")
                # é‡åˆ°å¼‚å¸¸ç­‰å¾…5åˆ†é’Ÿåé‡è¯•
                logger.info("â³ ç­‰å¾…5åˆ†é’Ÿåé‡è¯•...")
                await asyncio.sleep(5 * 60)
                
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
    finally:
        await sync_service.stop()
        logger.info("âœ… åŒæ­¥æœåŠ¡å·²åœæ­¢")


async def run_manual_sync():
    """è¿è¡Œä¸€æ¬¡æ€§æ‰‹åŠ¨åŒæ­¥"""
    logger.info("âš¡ æ‰§è¡Œæ‰‹åŠ¨åŒæ­¥...")
    sync_service = SyncService()
    try:
        await sync_service.initialize()
        success = await sync_service.run_manual_sync()
        if success:
            logger.info("âœ… æ‰‹åŠ¨åŒæ­¥å®Œæˆ")
            # ç”ŸæˆJSONç¼“å­˜
            await generate_cache()
        else:
            logger.error("âŒ æ‰‹åŠ¨åŒæ­¥å¤±è´¥")
            sys.exit(1)
    finally:
        await sync_service.stop()


async def run_force_full_sync():
    """å¼ºåˆ¶æ‰§è¡Œå…¨é‡åŒæ­¥ï¼ˆæ¸…ç©ºNeo4jæ•°æ®åé‡æ–°åŒæ­¥ï¼‰"""
    logger.info("ğŸ§¹ å¼ºåˆ¶å…¨é‡åŒæ­¥ï¼šæ¸…ç©ºNeo4jæ•°æ®...")
    
    from core.graphiti_client import GraphitiClient
    
    # æ¸…ç©ºNeo4jæ•°æ®
    graph_client = GraphitiClient()
    try:
        await graph_client.initialize()
        
        # åˆ é™¤æ‰€æœ‰NotionPageèŠ‚ç‚¹å’ŒSyncMetadata
        clear_queries = [
            "MATCH (n:NotionPage) DETACH DELETE n",
            "MATCH (m:SyncMetadata) DELETE m"
        ]
        
        async with graph_client._driver.session() as session:
            for query in clear_queries:
                result = await session.run(query)
                summary = await result.consume()
                logger.info(f"æ¸…ç†å®Œæˆï¼šåˆ é™¤äº† {summary.counters.nodes_deleted} ä¸ªèŠ‚ç‚¹")
        
        await graph_client.close()
        logger.info("ğŸ§¹ Neo4jæ•°æ®å·²æ¸…ç©º")
        
    except Exception as e:
        logger.error(f"âŒ æ¸…ç©ºNeo4jæ•°æ®å¤±è´¥: {e}")
        return
    
    # ç°åœ¨è¿è¡ŒåŒæ­¥ï¼ˆå°†è§¦å‘å…¨é‡åŒæ­¥ï¼‰
    logger.info("ğŸ”„ å¼€å§‹å…¨é‡åŒæ­¥...")
    sync_service = SyncService()
    try:
        await sync_service.initialize()
        success = await sync_service.run_manual_sync()
        if success:
            logger.info("âœ… å¼ºåˆ¶å…¨é‡åŒæ­¥å®Œæˆ")
            # ç”ŸæˆJSONç¼“å­˜
            await generate_cache()
        else:
            logger.error("âŒ å¼ºåˆ¶å…¨é‡åŒæ­¥å¤±è´¥")
            sys.exit(1)
    finally:
        await sync_service.stop()


async def show_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    logger.info("ğŸ“Š æ£€æŸ¥ç³»ç»ŸçŠ¶æ€...")
    
    try:
        sync_service = SyncService()
        await sync_service.initialize()
        
        stats = await sync_service.get_stats()
        logger.info(f"åŒæ­¥æœåŠ¡çŠ¶æ€: {stats}")
        
        await sync_service.stop()
        
    except Exception as e:
        logger.error(f"æ— æ³•è·å–çŠ¶æ€: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Chimera åŒæ­¥æœåŠ¡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python run_chimera.py                    # è¿è¡Œ15åˆ†é’ŸæŒç»­åŒæ­¥ç›‘æµ‹
  python run_chimera.py --manual-sync      # æ‰§è¡Œä¸€æ¬¡æ‰‹åŠ¨åŒæ­¥
  python run_chimera.py --status           # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
  python run_chimera.py --generate-cache   # ç”ŸæˆJSONç¼“å­˜æ–‡ä»¶
  python run_chimera.py --force-full-sync  # å¼ºåˆ¶å…¨é‡åŒæ­¥ï¼ˆæµ‹è¯•é¦–æ¬¡è¿è¡Œï¼‰

æ³¨æ„: MCPæœåŠ¡å™¨è¯·å•ç‹¬è¿è¡Œï¼š
  python fastmcp_server.py --port 3000
        """
    )
    
    parser.add_argument(
        "--manual-sync", 
        action="store_true",
        help="æ‰§è¡Œä¸€æ¬¡æ‰‹åŠ¨åŒæ­¥"
    )
    
    parser.add_argument(
        "--status", 
        action="store_true",
        help="æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"
    )
    
    parser.add_argument(
        "--debug", 
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ¨¡å¼"
    )
    
    parser.add_argument(
        "--generate-cache", 
        action="store_true",
        help="ç”ŸæˆJSONç¼“å­˜æ–‡ä»¶"
    )
    
    parser.add_argument(
        "--force-full-sync", 
        action="store_true",
        help="å¼ºåˆ¶æ‰§è¡Œå…¨é‡åŒæ­¥ï¼ˆæ¸…ç©ºNeo4jæ•°æ®åé‡æ–°åŒæ­¥ï¼‰"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸ”„ Chimera åŒæ­¥æœåŠ¡")
    logger.info("=" * 60)
    
    # æ£€æŸ¥é…ç½®
    settings = get_settings()
    if args.debug:
        logger.info(f"é…ç½®: Neo4j URI: {settings.neo4j_uri}")
    
    # æ ¹æ®å‚æ•°è¿è¡Œç›¸åº”çš„åŠŸèƒ½
    try:
        if args.manual_sync:
            asyncio.run(run_manual_sync())
        elif args.status:
            asyncio.run(show_status())
        elif args.generate_cache:
            asyncio.run(generate_cache())
        elif args.force_full_sync:
            asyncio.run(run_force_full_sync())
        else:
            # é»˜è®¤è¿è¡ŒæŒç»­åŒæ­¥
            asyncio.run(run_continuous_sync())
            
    except Exception as e:
        logger.exception(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()